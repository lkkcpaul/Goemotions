{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484d3e3b-85cc-4990-bf88-25f9a3d7bfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkkcp\\anaconda3\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed864baf-b5c0-4ca2-8bec-d9dd96c23824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2144723c-3595-4889-b8e2-9c46e40bedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo2idx = {}\n",
    "idx2emo = {}\n",
    "i = 0\n",
    "with open('./data/emotions.txt','r') as f:\n",
    "    for line in f:\n",
    "        emo2idx[line.strip()]=i\n",
    "        idx2emo[i] = line.strip()\n",
    "        i+=1\n",
    "emo_list = list(emo2idx.keys())\n",
    "emo_list.remove('neutral')\n",
    "#emo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070b4b19-8e99-44db-905a-03ab4f54889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.tsv\",sep='\\t', header = None, names=['Text','Class','ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "842701f1-e87b-4663-a955-af2612d4fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_process(df):\n",
    "    sample_size = df.shape[0]\n",
    "    z = pd.DataFrame(np.zeros((sample_size,len(emo_list)),int),index=df.index, columns=emo_list)\n",
    "    res_df = pd.concat([df,z],axis=1)\n",
    "    #res_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    class_list = df['Class'].map(lambda x:x.split(','))#.tolist()\n",
    "    for i in class_list.index:\n",
    "        for c in class_list.loc[i]:\n",
    "            res_df.loc[i,idx2emo[int(c)]]=1\n",
    "        \n",
    "    print('Dropped', res_df['neutral'].sum(), 'samples that are neutral')\n",
    "        \n",
    "    res_df.drop(res_df[res_df['neutral']==1].index,inplace=True)\n",
    "    res_df.drop(['Class','neutral'],axis=1,inplace=True)\n",
    "    res_df.reset_index(drop=True,inplace=True)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6cb58c05-51ef-4356-a98f-531d169c1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 76.0 samples that are neutral\n",
      "Dropped 68.0 samples that are neutral\n",
      "Dropped 62.0 samples that are neutral\n"
     ]
    }
   ],
   "source": [
    "small_sample_size = 200\n",
    "small_train_df = df_process(train_df[['Text','Class']].head(small_sample_size))\n",
    "small_val_df = df_process(train_df.loc[small_sample_size:small_sample_size*2-1, ['Text','Class']])\n",
    "small_test_df = df_process(train_df.loc[small_sample_size*2:small_sample_size*3-1, ['Text','Class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "4ea8b025-7062-486e-8f27-1bfb6b73a008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  admiration  amusement  \\\n",
       "0                     WHY THE FUCK IS BAYLESS ISOING           0          0   \n",
       "1                        To make her feel threatened           0          0   \n",
       "2                             Dirty Southern Wankers           0          0   \n",
       "3  OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...           0          0   \n",
       "4  Yes I heard abt the f bombs! That has to be wh...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  joy  \\\n",
       "0      1          0         0       0          0          0       0  ...    0   \n",
       "1      0          0         0       0          0          0       0  ...    0   \n",
       "2      0          1         0       0          0          0       0  ...    0   \n",
       "3      0          0         0       0          0          0       0  ...    0   \n",
       "4      0          0         0       0          0          0       0  ...    0   \n",
       "\n",
       "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0     0            0         0      0            0       0        0        0   \n",
       "1     0            0         0      0            0       0        0        0   \n",
       "2     0            0         0      0            0       0        0        0   \n",
       "3     0            0         0      0            0       0        0        0   \n",
       "4     0            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         1  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "64ae3430-38e2-4d4c-a33b-717e887294bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_df.to_csv('data/small_train.csv',index=False)\n",
    "small_val_df.to_csv('data/small_val.csv',index=False)\n",
    "small_test_df.to_csv('data/small_test.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2bfbdc4b-0834-4c3b-9b48-b762dfc78c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/lkkcp/.cache/huggingface/datasets/csv/default-3db98f2b8151b599/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1003.02it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/lkkcp/.cache/huggingface/datasets/csv/default-3db98f2b8151b599/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 370.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\",data_files=\n",
    "                       {\"train\": \"data/small_train.csv\",\n",
    "                        \"validation\": \"data/small_val.csv\",\n",
    "                         \"test\": \"data/small_test.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "7fed779b-1468-4bab-a4a5-1e86383ea930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Text', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise'],\n",
       "        num_rows: 132\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d4232441-dfe3-4144-8db2-dd5bc8d74b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-uncased'\n",
    "MAXLEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8563a72d-b7af-4575-a78a-51187971735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 9.35kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 286kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 77.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.52MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "12e8fa97-fba3-4b1b-8887-da0cfc0a5b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\lkkcp\\.cache\\huggingface\\datasets\\csv\\default-3db98f2b8151b599\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-9f2ddb17391f21b0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\lkkcp\\.cache\\huggingface\\datasets\\csv\\default-3db98f2b8151b599\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-701394d76a1564e9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\lkkcp\\.cache\\huggingface\\datasets\\csv\\default-3db98f2b8151b599\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-7ddd3fc03db9c072.arrow\n",
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "def encode(samples):\n",
    "    return tokenizer(samples['Text'], padding=\"max_length\", truncation=True, max_length=MAXLEN)\n",
    "def label_formatting(samples):\n",
    "    labels_matrix = np.zeros((len(samples['Text']), len(emo_list)))\n",
    "    for i, emo in enumerate(emo_list):\n",
    "        labels_matrix[:,i] = samples[emo]\n",
    "    return {'labels':labels_matrix.tolist()}\n",
    "\n",
    "temp = dataset.map(encode,batched=True)\n",
    "encoded_dataset = temp.map(label_formatting,batched=True,remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "01030896-d622-4e2a-b3dc-0339c60b3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2f864981-f953-4279-95b1-62d4a4e56ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2emo_no_neutral = idx2emo.copy()\n",
    "emo2idx_no_neutral = emo2idx.copy()\n",
    "del idx2emo_no_neutral[27]\n",
    "del emo2idx_no_neutral['neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c71ed861-9444-4bc5-9881-e3859d070c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [00:07<00:00, 62.1MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=len(emo_list), problem_type=\"multi_label_classification\",\n",
    "    id2label=idx2emo_no_neutral, label2id=emo2idx_no_neutral\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "98234926-495d-4072-9924-dbb021cf144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "18b7fddb-8ff8-4cf5-98c0-b92b7a3838e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "05e2d3f7-2e99-4e17-84e6-fb089476e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "388a4c40-d689-4d96-9c33-f4d9e440c94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9c103ce3-1686-429c-b290-527cda7e6cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2339,  1996,  6616,  2003,  3016,  3238, 11163,  2075,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ffb85e89-e97d-47db-a088-0792ddb8419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7079, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.0056,  0.4173,  0.6280, -0.0743,  0.1634,  0.1295, -0.6982,  0.2129,\n",
       "          0.3269,  0.2269, -0.1872,  0.0512,  0.3449,  0.1009, -0.0137,  0.1944,\n",
       "         -0.2084, -0.2642,  0.0546, -0.4121, -0.6768, -0.5645,  0.5141,  0.1986,\n",
       "          0.2496,  0.1539,  0.4166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "18130feb-46e8-4f50-9f95-cae9321562fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "885adf12-52aa-4265-8ffd-3fb6187d823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkkcp\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 2:11:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.620903</td>\n",
       "      <td>0.059979</td>\n",
       "      <td>0.477740</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.582016</td>\n",
       "      <td>0.036125</td>\n",
       "      <td>0.470312</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=0.6377503275871277, metrics={'train_runtime': 8044.2154, 'train_samples_per_second': 0.031, 'train_steps_per_second': 0.004, 'total_flos': 16316547102720.0, 'train_loss': 0.6377503275871277, 'epoch': 2.0})"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19380c86-72d6-465c-afa0-0ff2a5f01b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "4f92d68a-d233-4d0d-bc7a-475b657ff8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 22:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.620903491973877,\n",
       " 'eval_f1': 0.059979317476732165,\n",
       " 'eval_roc_auc': 0.4777397228261784,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 1468.2067,\n",
       " 'eval_samples_per_second': 0.09,\n",
       " 'eval_steps_per_second': 0.012,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
